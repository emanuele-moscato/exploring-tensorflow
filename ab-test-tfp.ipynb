{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian A/B testing with TensorFlow Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Chapter2_MorePyMC/Ch2_MorePyMC_TFP.ipynb\n",
    "\n",
    "In case migration is needed from TensorFlow 1.xx to TensorFlow 2: https://www.tensorflow.org/guide/migrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from plotly.offline import iplot, init_notebook_mode\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "tfd = tfp.distributions\n",
    "tfb = tfp.bijectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_p = tfd.Uniform(low=0., high=1., name=\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_p.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_true = 0.05\n",
    "n_samples = 1500\n",
    "\n",
    "occurrences = tfd.Bernoulli(probs=prob_true).sample(sample_shape=n_samples)\n",
    "\n",
    "print(f\"True frequency (probability): {prob_true}\")\n",
    "print(f\"Observed frequency: {tf.reduce_sum(occurrences) / n_samples}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a log prob for the product of the likelihood times the posterior for a value for the probability of an occurrence and samples of occurrences. In our case we take\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "p_a \\sim p(p_a) = \\text{Uniform}(0, 1) \\\\\n",
    "X \\sim p(x | p_a) = \\text{Ber}(x | p_a)\\,.\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The data $\\mathcal{D} = \\left\\lbrace x_1, \\ldots, x_N \\right\\rbrace$ is assumed to be i.i.d., so the likelihood is the product of the probability of each value,\n",
    "$$\n",
    "p\\left( \\mathcal{D} | p_a \\right) = \\prod_{i=1}^{N} p(x_i | p_a) = \\prod_{i=1}^{N} \\text{Ber}(x_i | p_a)\n",
    "$$\n",
    "\n",
    "so the posterior probability distribution $p\\left( p_a | \\mathcal{D} \\right)$ of $p_a$ given the data is\n",
    "$$\n",
    "p\\left( p_a | \\mathcal{D} \\right) \\simeq p\\left( \\mathcal{D} | p_a \\right)\\, p(p_a) = \\prod_{i=1}^{N} \\text{Ber}(x_i | p_a)\\, p(p_a),\n",
    "$$\n",
    "where the approximation lies in the fact that we are ignoring the evidence term $p\\left(\\mathcal{D}\\right)$ that should be at the denominator of the RHS(s).\n",
    "\n",
    "The MCMC uses the log of the joint probability above,\n",
    "$$\n",
    "\\log\\left( p\\left( p_a | \\mathcal{D} \\right) \\right) = \\log\\left( p(p_a) \\right) + \\sum_{i=1}^N \\log\\left( \\text{Ber}(x_i | p_a) \\right)\\,,\n",
    "$$\n",
    "\n",
    "and the logic is, at each step of the chain, to evaluate it on the new proposed values for $p_a$ keeping the occurrences $\\left\\lbrace x_i \\right\\rbrace_i$ fixed and use the resulting value to compute the probability of acceptance of the new value for $p_a$.\n",
    "\n",
    "TFP allows to define random variables for $p(p_a)$ and $p(x | p_a)$, which expose the `log_prob()` method adding which it is possible to define the joint log probabilty above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_log_prob(occurrences, prob_a):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Prior for probability of an occurrence.\n",
    "    rv_prob_a = tfd.Uniform(low=0., high=1.)\n",
    "    \n",
    "    # Likelihood for the occurrences given probability\n",
    "    # prob_a for an occurrence.\n",
    "    rv_occurrences = tfd.Bernoulli(probs=prob_a)\n",
    "    \n",
    "    # Log probability of prior * likelihood. Likelihood is\n",
    "    # decomposed in the product of the probabilities of each\n",
    "    # occurrence (i.i.d. hypothesis).\n",
    "    return rv_prob_a.log_prob(prob_a) + tf.reduce_sum(rv_occurrences.log_prob(occurrences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_steps = 48000\n",
    "burnin = 25000 \n",
    "leapfrog_steps=2\n",
    "\n",
    "# Set the chain's start state.\n",
    "initial_chain_state = [\n",
    "    tf.reduce_mean(tf.dtypes.cast(occurrences, dtype=tf.float32))\n",
    "    * tf.ones([], dtype=tf.float32, name=\"init_prob_A\")\n",
    "]\n",
    "\n",
    "# Since HMC operates over unconstrained space, we need to transform the\n",
    "# samples so they live in real-space.\n",
    "unconstraining_bijectors = [\n",
    "    tfp.bijectors.Identity()   # Maps R to R.  \n",
    "]\n",
    "\n",
    "# Define a closure over our joint_log_prob.\n",
    "# The closure makes it so the HMC doesn't try to change the `occurrences` but\n",
    "# instead determines the distributions of other parameters that might generate\n",
    "# the `occurrences` we observed.\n",
    "unnormalized_posterior_log_prob = lambda *args: joint_log_prob(occurrences, *args)\n",
    "\n",
    "# Initialize the step_size. (It will be automatically adapted.)\n",
    "# Old TensorFlow 1.xx code.\n",
    "# with tf.variable_scope(tf.get_variable_scope(), reuse=tf.AUTO_REUSE):\n",
    "#     step_size = tf.get_variable(\n",
    "#         name='step_size',\n",
    "#         initializer=tf.constant(0.5, dtype=tf.float32),\n",
    "#         trainable=False,\n",
    "#         use_resource=True\n",
    "#     )\n",
    "\n",
    "# Older TFP version code.\n",
    "# hmc = tfp.mcmc.TransformedTransitionKernel(\n",
    "#     inner_kernel=tfp.mcmc.HamiltonianMonteCarlo(\n",
    "#         target_log_prob_fn=unnormalized_posterior_log_prob,\n",
    "#         num_leapfrog_steps=leapfrog_steps,\n",
    "#         step_size=step_size,\n",
    "#         step_size_update_fn=tfp.mcmc.make_simple_step_size_update_policy(num_adaptation_steps=int(burnin * 0.8)),\n",
    "#         state_gradients_are_stopped=True\n",
    "#     ),\n",
    "#     bijector=unconstraining_bijectors\n",
    "# )\n",
    "\n",
    "#Â When using bijectors and an step size update policy (now available as\n",
    "# a kernel instead of a function), the sequence (from inner to outer\n",
    "# kernel) is:\n",
    "# mcmc engine (HMC) kernel --> bijectors kernel --> step size adaptation kernel.\n",
    "hmc = tfp.mcmc.HamiltonianMonteCarlo(\n",
    "        target_log_prob_fn=unnormalized_posterior_log_prob,\n",
    "        num_leapfrog_steps=leapfrog_steps,\n",
    "        step_size=tf.constant(0.01, dtype=tf.float32)\n",
    "    )\n",
    "\n",
    "hmc = realnvp_hmc = tfp.mcmc.TransformedTransitionKernel(\n",
    "    inner_kernel=hmc,\n",
    "    bijector=tfb.Identity()\n",
    ")\n",
    "\n",
    "hmc = tfp.mcmc.SimpleStepSizeAdaptation(\n",
    "    inner_kernel=hmc,\n",
    "    num_adaptation_steps=int(burnin * 0.8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Warnings from the commented version:\n",
    "\n",
    "```WARNING:tensorflow:From <ipython-input-132-c435a30efc4b>:44: make_simple_step_size_update_policy (from tensorflow_probability.python.mcmc.hmc) is deprecated and will be removed after 2019-05-22.\n",
    "Instructions for updating:\n",
    "Use tfp.mcmc.SimpleStepSizeAdaptation instead.```\n",
    "\n",
    "```/Users/emanuele_moscato/anaconda3/envs/tf-test/lib/python3.7/site-packages/tensorflow_probability/python/mcmc/internal/util.py:494: UserWarning: `step_size` is not a `tf.Tensor`, Python number, or Numpy array. If this parameter is mutable (e.g., a `tf.Variable`), then the behavior implied by `store_parameters_in_results` will silently change on 2019-08-01. Please consult the docstring for `store_parameters_in_results` details and use `store_parameters_in_results=True` to silence this warning.\n",
    "  param_name))```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "posterior_prob_A_, (is_accepted, log_accept_ratio, inner_results) = tfp.mcmc.sample_chain(\n",
    "    num_results=number_of_steps,\n",
    "    num_burnin_steps=burnin,\n",
    "    current_state=initial_chain_state,\n",
    "    kernel=hmc,\n",
    "    trace_fn=lambda _, pkr: (\n",
    "        pkr.inner_results.inner_results.is_accepted,\n",
    "        pkr.inner_results.inner_results.log_accept_ratio,\n",
    "        pkr.inner_results\n",
    "    )\n",
    ")\n",
    "\n",
    "# posterior_prob_A_, is_accepted, log_accept_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reduce_mean(posterior_prob_A_[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = go.Histogram(\n",
    "    x=posterior_prob_A_[0].numpy(),\n",
    "    histnorm=\"probability density\"\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace])\n",
    "\n",
    "iplot(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
