{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will broadly follow the topics covered in the [Tensorflow introductory guide](https://www.tensorflow.org/guide/low_level_intro)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eager execution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow now support [eager execution](https://www.tensorflow.org/tutorials/eager/eager_basics) (execution on-the-fly), which does not require the definition of a graph structure and placeholders and a compiling step anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamental Tensorflow objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3 fundamental objects that Tensorflow provides:\n",
    "- Tensors (see [the documentation](https://www.tensorflow.org/api_docs/python/tf/Tensor) and [a guide](https://www.tensorflow.org/guide/tensors)),\n",
    "- Sessions,\n",
    "- Operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generalities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensors are multidimensional arrays that work as inputs or outputs of operations. Tensorflow has fast routines to manipulate them.\n",
    "\n",
    "There are special types of tensors, such as `tf.Variable`, `tf.constant`, `tf.placeholder` and `tf.SparseTensor`.\n",
    "\n",
    "All tensors apart from `tf.Variable` are immutable (but their values may vary if they are the result of different runs of an operations, with different inputs). Before eager execution was introduced in Tensorflow, tensors were literally just handles representing the edges of a graph without a particular value, which was only present when the graph was actually evaluated. With eager execution, though, tensors can now have values outside the context of the evaluation of the graph.\n",
    "\n",
    "__Eager execution:__ the eager execution option has to be selected \"at program startup\" (right after the initial import statements). If it is not, the tensors will not actually have any value for their component, as without eager execution we need to be in the context of a Tensorflow session and evaluate the tensors explicitly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = tf.constant([[1, 2], [3, 4]]) # A 2x2 matrix\n",
    "m2 = tf.constant([[1, 0], [0, 1]]) # The 2x2 identity matrix\n",
    "\n",
    "print(m1)\n",
    "print(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensors have a shape\n",
    "print(tf.constant([[12, 3], [12, 2]], dtype=tf.int32).shape)\n",
    "print(tf.constant([[2], [4], [6]]).shape)\n",
    "\n",
    "# In Tensorflow, the rank of a tensor is the number of\n",
    "# its dimensions (the number of indices)\n",
    "print(tf.rank(tf.Variable([[1, 2], [1, 3], [8, 9]])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow implements matrix multiplication\n",
    "a = tf.constant([[2], [4], [5]])\n",
    "b = tf.constant([[1, 0, 0]])\n",
    "\n",
    "print(f\"a={a}\")\n",
    "print(f\"b={b}\")\n",
    "print(tf.matmul(a, b))\n",
    "print(f\"shape(a)={a.shape}\")\n",
    "print(f\"shape(b)={b.shape}\")\n",
    "print(f\"shape(a*b)={tf.matmul(a, b).shape}\")\n",
    "\n",
    "# Tensorflow can also return the shape of a tensor\n",
    "# as another tensor, which can be used at runtime,\n",
    "# even if the shapes change dynamically\n",
    "print(tf.shape(a))\n",
    "\n",
    "# Tensors' components can be accessed by the same\n",
    "# indexing as NumPy arrays\n",
    "print(a[0,0])\n",
    "\n",
    "# Addition and multiplication of tensors can also\n",
    "# happen with the + and * operators\n",
    "print(m1*m1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorflow Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variables (`tf.Variable`) are tensors whose value can be changed by operations performed on them and (if not in eager execution mode) it can exists outside of the context of a session.\n",
    "\n",
    "Variables can be initialized with `tf.get_variable()`, specifying a variable name and shape. The variable created this way has its value randomly initialized (with the `tf.glorot_uniform_initializer`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "variable_1 = tf.get_variable(\"first_variable\", (3, 2))\n",
    "\n",
    "print(variable_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Placeholders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Placeholders (`tf.placeholder`) are an abstraction around tensors that will be passed to an operations. If the operation is a sum of tensors, we can pass it tensors for which we specify values (that will be taken when the graph is run, if eager execution is not active) or we can pass it two placeholder that \"promise\" that a specific number of tensors with a specific shape will be passed as input to the operation itself.\n",
    "\n",
    "We don't even have to specify the shape of the placeholder tensor: we'll just get an error if the operation performed on the values actually fed to the operation when the graph is run are inconsistent. This also means that the dimension of the output tensor from the operation is a priori unknown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Placeholders\n",
    "p1 = tf.placeholder(dtype=tf.int32)\n",
    "p2 = tf.placeholder(dtype=tf.int32)\n",
    "\n",
    "# Operation\n",
    "op = tf.matmul(p1, p2)\n",
    "\n",
    "print(op)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sessions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Tensorflow session can be thought of as an executable, an object that executes a graph, performing operations among tensors and giving them specific values.\n",
    "\n",
    "__Note:__ Tensorflow variables cannot be passed arbitrary values by hand, so if we want to build a simple graph explicitly we have to use `tf.constant`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a graph\n",
    "v1 = tf.constant([[1, 1], [3, 1]])\n",
    "v2 = tf.constant([[1, 0], [0, 1]])\n",
    "\n",
    "prod = tf.matmul(v1, v2) # Operation between tensors, returning another tensor\n",
    "\n",
    "# Evaluate the graph\n",
    "sess.run(prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all operations return a tensor when run. Some of them are run only to cause side effects, such as initializing tensors. In this cases, running them executes the side effect but returns `None`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a variable to initialize\n",
    "var_to_initialize = tf.get_variable(\"to_init\", (4,2))\n",
    "\n",
    "# Instantiate an initializer\n",
    "initializer = tf.global_variables_initializer()\n",
    "\n",
    "# Run the initialization step\n",
    "print(sess.run(initializer))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a graph has placeholders as input, we pas explicit values for them and the value of the output will be computed when the the graph is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(\n",
    "    op, \n",
    "    feed_dict = {\n",
    "        p1: [[1], [1], [2], [4]],\n",
    "        p2: [[1, 3, 5, 9]]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `tf.data` module provides richer ways to treat the input to a graph than placeholders.\n",
    "\n",
    "We can recast data into a Tensorflow iterator (`tf.data.Iterator`) and get the next sample by calling its `get_next()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define some data\n",
    "data = [\n",
    "    [1.0],\n",
    "    [2.0],\n",
    "    [3.0],\n",
    "    [4.0]\n",
    "]\n",
    "\n",
    "# Create slices from the data\n",
    "slices = tf.data.Dataset.from_tensor_slices(data)\n",
    "\n",
    "# Create a one-shot iterator and get the samples\n",
    "next_item = slices.make_one_shot_iterator().get_next()\n",
    "\n",
    "# Have a session run the operation to get the data\n",
    "while True:\n",
    "    try:\n",
    "        print(sess.run(next_item))\n",
    "    except:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Layers (`tf.layers`) are the building block for the graph. They add __trainable parameters__ to it, those numbers over which we will optimize while training the neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a simple graph with an input x (as a placeholder) that is a vector with three components, and a linear fully connected dense layer that gives an output y.\n",
    "\n",
    "The `shape` option of the placeholder specifies the shape of the input. In this case, `(None, n)` stands for an input with shape `(n_samples, n)`, where `n_samples` is not specified.\n",
    "\n",
    "The `unit` option to the layer object is represents the dimension of the output and given this and the dimension of the input the layer automatically determines the dimension of the matrix of weights that maps input to output.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholder for input\n",
    "x = tf.placeholder(dtype=tf.float32, shape=(None, 3))\n",
    "\n",
    "# Create a model with a single layer\n",
    "linear_model = tf.layers.Dense(units=1)\n",
    "\n",
    "# Create the output\n",
    "y = linear_model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weights inside the layer must be initialized. To do so, we have to have a session run an initializer. The initialized values will remain valid only within that session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of our linear model can now be run (evaluated) given a value for the input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One single sample as the input\n",
    "print(sess.run(\n",
    "    y,\n",
    "    feed_dict = {x: [[0.1, 0.4, 12.9]]}\n",
    "))\n",
    "\n",
    "# Two samples as the input\n",
    "print(sess.run(\n",
    "    y,\n",
    "    feed_dict = {x: [[0.1, 0.4, 12.9], [12.2, 1.1, 3.0]]}\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow also offers a function associated to each layer that instantiates and executes the layer in one go, taking the input tensor as an input. The example for a `Dense` layer would be a the `tf.layers.dense()` function.\n",
    "\n",
    "__Note:__ the following cell also runs the initialization routine, which gives different values for the weights in the dense layer every time it's called. Therefore, the output is different every time the cell is run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define input tensor (placeholder)\n",
    "z_in = tf.placeholder(dtype=tf.float32, shape=(None, 3))\n",
    "\n",
    "# Define output tensor, including the intermediate layer\n",
    "z_out = tf.layers.dense(z_in, units=1)\n",
    "\n",
    "# Initialize global variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Execute the graph\n",
    "print(sess.run(\n",
    "    z_out,\n",
    "    feed_dict = {z_in: [[1.2, 23.4, 44.1]]}\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training a neural network means optimizing the internal values (weights) of the layers according to some criterion, i.e. minimizing a loss function. The loss function maps the predictions made on the training data and the true target value of the training data to a real number: the predictions depend on the weight inside the layers, and we adjust them to get a minimum of the function. To do this, we use an optimizer.\n",
    "\n",
    "Let's build a simple model and train it on the iris dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_data_input = tf.constant(iris_data['data'])\n",
    "iris_data_labels = tf.constant(iris_data['target'])\n",
    "\n",
    "print(iris_data_input.eval(session=sess)[:10,:])\n",
    "print(iris_data_labels.eval(session=sess)[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the graph. Notice that the output layer has 4 units, as we are going to one-hot encode our 4 labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_input = tf.placeholder(dtype=tf.float64, shape=(None, 4))\n",
    "\n",
    "linear_layer = tf.layers.Dense(units=4)\n",
    "\n",
    "y_pred = linear_layer(nn_input)\n",
    "\n",
    "# Get a prediction with randomly initialized weights for the\n",
    "# first 10 samples.\n",
    "sess.run(tf.global_variables_initializer())\n",
    "sess.run(\n",
    "    y_pred,\n",
    "    feed_dict = {nn_input: iris_data_input.eval(session=sess)}\n",
    ")[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a loss functions. A common loss for multiclass classification problems is the __softmax categorical cross entropy__, which requires that the labels are one-hot encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encode the labels\n",
    "y_true = tf.one_hot(iris_data_labels, depth=4)\n",
    "\n",
    "print(y_true.eval(session=sess)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Define the loss function as a function of the labels\n",
    "# and the predictions\n",
    "loss = tf.losses.softmax_cross_entropy(y_true, y_pred)\n",
    "\n",
    "# Compute loss given the current (randomly initialized)\n",
    "# values for the weights\n",
    "print(sess.run(\n",
    "    loss,\n",
    "    feed_dict={nn_input: iris_data_input.eval(session=sess)}\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model. In order to train out model we need to choose an optimizer and have it minimize the loss function w.r.t. the parameters (weights) in the layers. In this case we proceed with a standard __gradient descent__ optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate an optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "# Create a training operation\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# Train the model\n",
    "loss_values = []\n",
    "n_epochs = 300\n",
    "for i in range(n_epochs):\n",
    "    _, loss_value = sess.run(\n",
    "        (train, loss),\n",
    "        feed_dict = {nn_input: iris_data_input.eval(session=sess)}\n",
    "    )\n",
    "    loss_values.append(loss_value)\n",
    "    if i%20==0:\n",
    "        print(loss_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the values of the loss function for each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = go.Scatter(\n",
    "    x=list(range(len(loss_values))),\n",
    "    y=loss_values,\n",
    "    mode='markers'\n",
    ")\n",
    "\n",
    "layout = go.Layout(\n",
    "    xaxis=dict(\n",
    "        title='epoch'\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='loss function value'\n",
    "    )\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace], layout=layout)\n",
    "\n",
    "iplot(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
