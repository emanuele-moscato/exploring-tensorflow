{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some TensorFlow 2.0 and TensorFlow Probability basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on: https://github.com/tensorflow/probability/blob/master/tensorflow_probability/examples/jupyter_notebooks/A_Tour_of_TensorFlow_Probability.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import numpy as np\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "init_notebook_mode(connected=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow: linear algebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As long as there are no for loops, all the operations are automatically vectorizes (and thus more performant)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solving a linear system\n",
    "$$\n",
    "y = m x\n",
    "$$\n",
    "for $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = tf.random.uniform(shape=[10, 10])\n",
    "\n",
    "y = tf.random.uniform(shape=[10, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.linalg.solve(\n",
    "    matrix=m,\n",
    "    rhs=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(\n",
    "    tf.linalg.matmul(m, x).numpy(),\n",
    "    y.numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can of course be achieved by inverting $m$ by hand (if it's invertible!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.linalg.det(m) != 0:\n",
    "    minv = tf.linalg.inv(m)\n",
    "    \n",
    "    x_alternative = tf.linalg.matmul(minv, y)\n",
    "\n",
    "else:\n",
    "    print(\"Matrix m is not invertible!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(\n",
    "    x_alternative.numpy(),\n",
    "    x.numpy()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define sets of tensors stacking them teogether using another dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_stacked = tf.random.uniform(shape=(5, 10, 10))\n",
    "\n",
    "y_stacked = tf.random.uniform(shape=(5, 10, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Invert each matrix.\n",
    "m_stacked_inv = tf.linalg.inv(m_stacked)\n",
    "\n",
    "m_stacked_inv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_stacked = tf.linalg.matmul(m_stacked_inv, y_stacked)\n",
    "\n",
    "x_stacked.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automatic differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiating\n",
    "$$\n",
    "b = \\frac{1}{2}\\,a^2\n",
    "$$\n",
    "w.r.t. $a$ and setting $a = 29$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(29.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([a])\n",
    "    \n",
    "    b = 0.5 * a**2\n",
    "    \n",
    "grad = tape.gradient(b, a)\n",
    "\n",
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiating w.r.t. multiple variables (proper gradient) the function\n",
    "$$\n",
    "F(a, b) = a\\,\\sin^b(b)\n",
    "$$\n",
    "and setting $a=1$ and $b=\\pi/2$.\n",
    "\n",
    "Result:\n",
    "$$\n",
    "\\nabla F(a, b) = \\left(\\begin{array}{c}\n",
    "\\sin^b(b) \\\\\n",
    "a\\,b \\cos(b)\n",
    "\\end{array}\\right) =\n",
    "\\left(\\begin{array}{c}\n",
    "1 \\\\\n",
    "0\n",
    "\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant(1.)\n",
    "b = tf.constant(np.pi/2.)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch([a, b])\n",
    "    \n",
    "    f = a * (tf.sin(b)) ** b\n",
    "\n",
    "grad = tape.gradient(f, [a, b])\n",
    "\n",
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Differentiating w.r.t. vectors: let\n",
    "\n",
    "$$\n",
    "\\mathbf{a} = \\left(\\begin{array}{r}\n",
    "1 \\\\\n",
    "2 \\\\\n",
    "3\n",
    "\\end{array}\\right) \\equiv\n",
    "\\left(\\begin{array}{r}\n",
    "a_1 \\\\\n",
    "a_2 \\\\\n",
    "a_3\n",
    "\\end{array}\\right)\\in \\mathbb{R}^3,\\quad\n",
    "b = \\left(\\begin{array}{ccc}\n",
    "1 & 0 & 0\\\\\n",
    "0 & -1 & 0 \\\\\n",
    "0 & 0 & 1\n",
    "\\end{array}\\right) \\equiv\n",
    "\\left(\\begin{array}{ccc}\n",
    "b_{11} & b_{12} & b_{13} \\\\\n",
    "b_{21} & b_{22} & b_{23} \\\\\n",
    "b_{31} & b_{32} & b_{33}\n",
    "\\end{array}\\right)\\in \\text{Mat}_{\\mathbb{R}}(3)\n",
    "$$\n",
    "\n",
    "and consider the differentiation of the product $h(\\mathbf{a}) = b\\,\\mathbf{a}$ w.r.t. (the components of) $\\mathbf{a}$, setting $\\mathbf{a}$ to the above value in the end\n",
    "\n",
    "$$\n",
    "\\nabla h(\\mathbf{a}) = \\left(\\begin{array}{r}\n",
    "\\partial_{a_1} h(a_1, a_2, a_3) \\\\\n",
    "\\partial_{a_2} h(a_1, a_2, a_3) \\\\\n",
    "\\partial_{a_3} h(a_1, a_2, a_3)\n",
    "\\end{array}\\right) =\n",
    "\\left(\\begin{array}{r}\n",
    "b_{11} \\\\\n",
    "b_{22} \\\\\n",
    "b_{33}\n",
    "\\end{array}\\right) =\n",
    "\\left(\\begin{array}{r}\n",
    "1 \\\\\n",
    "-1 \\\\\n",
    "1\n",
    "\\end{array}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.constant([[1], [2], [3]], dtype=tf.float32)\n",
    "\n",
    "b = tf.constant([\n",
    "    [1, 0, 0],\n",
    "    [0, -1, 0],\n",
    "    [0, 0, 1]\n",
    "], dtype=tf.float32)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    tape.watch(a)\n",
    "    \n",
    "    g = tf.linalg.matmul(b, a)\n",
    "    \n",
    "grad = tape.gradient(g, a)\n",
    "\n",
    "grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement a gradient descent algorithm to find a minimum of\n",
    "\n",
    "$$\n",
    "F(x) = - x^2 + 2\\,x^4,\n",
    "$$\n",
    "\n",
    "where the (global) minima are $x_{1, 2} = \\pm1/2$. The algorithm will find either point, starting from $x=3$, where the value of the function is $1/8 = 0.125$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.constant(3, dtype=tf.float32)\n",
    "\n",
    "eps = 0.001\n",
    "\n",
    "n_iter = 1000\n",
    "\n",
    "x_values = [x.numpy()]\n",
    "f_values = []\n",
    "der_values = []\n",
    "\n",
    "for i in range(n_iter):\n",
    "    # print(f\"Iteration {i+1}\")\n",
    "    # print(\"-----------\")\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "\n",
    "        f = - x ** 2 + 2. * x ** 4\n",
    "        \n",
    "        f_values.append(f.numpy())\n",
    "\n",
    "    grad = tape.gradient(f, x)\n",
    "    \n",
    "    der_values.append(grad.numpy())\n",
    "\n",
    "    # print(f\"f'(x={x}): {grad}\")\n",
    "\n",
    "    x = x - grad * eps\n",
    "    \n",
    "    x_values.append(x.numpy())\n",
    "\n",
    "    # print(f\"x_new: {x}\\n\")\n",
    "\n",
    "f_values.append((- x ** 2 + 2. * x ** 4).numpy())\n",
    "\n",
    "print(\"Final values\")\n",
    "print(\"------------\")\n",
    "print(f\"(x, f(x)) = ({x_values[-1]}, {f_values[-1]}), f'(x) = {der_values[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = go.Scatter(\n",
    "    x=list(range(len(x_values))),\n",
    "    y=x_values,\n",
    "    mode=\"markers\"\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace])\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = go.Scatter(\n",
    "    x=x_values,\n",
    "    y=f_values,\n",
    "    mode=\"markers\",\n",
    "    marker=dict(\n",
    "        opacity=np.linspace(0.5, 1, len(x_values)),\n",
    "    ),\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace])\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorFlow probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental low-level object in TensorFlow Probability is the distribution, effectively a random number generator object. The two main mehtods of this class are `sample()` and `log_prob()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfd = tfp.distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal = tfd.Normal(loc=0., scale=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample the normal distribution object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_samples = normal.sample(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace = go.Histogram(\n",
    "    x=normal_samples.numpy(),\n",
    "    histnorm=\"probability\"\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=[trace])\n",
    "\n",
    "iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the log probability of a point in the domain of the distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal.log_prob(0.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the distribution is centered on 0 and symmetric, points that are symmetric w.r.t. 0 will have the same log probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal.log_prob([-1., 0., 1.])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distributions distinguish between a __batch shape__ and an __event shape__:\n",
    "- Batch shape = shape of a collection of distributions (possibly with different parameters)\n",
    "- Event shape = shape of a sample drawn from a distribution\n",
    "\n",
    "Batch shapes are put \"on the right\" while event shapes are put \"on the left\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we pass multiple values for the parameters of a distribution, a batch of distributions is automatically created (possibly broadcasting the values passed for the other parameters - if any - if shapes don't match). E.g., if we pass three values $\\mu_1, \\mu_2$ and $\\mu_3$ for the mean of a Normal distribution and only one value $\\sigma$ for its standard deviation, we create the batch of three Normal distributions\n",
    "\n",
    "$$\n",
    "\\left( \\mathcal{N}\\left(\\mu_1, \\sigma\\right), \\mathcal{N}\\left(\\mu_2, \\sigma\\right), \\mathcal{N}\\left(\\mu_3, \\sigma\\right) \\right)\n",
    "$$\n",
    "\n",
    "The distributions in the same batch are independent instances of the distribution class and the computations on them happen in parallel (as if they were vectorized arrays)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_batch = tfd.Normal(loc=[-5., 0., 5.], scale=1.)\n",
    "\n",
    "print(f\"Batch shape: {normal_batch.batch_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we then sample the batch of distribution $N$ times we get a tensor with shape $\\left( N, 3 \\right)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_batch_samples = normal_batch.sample(10000)\n",
    "\n",
    "print(f\"Shape of the sample: {normal_batch_samples.shape}\")\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(normal_batch.batch_shape[0]):\n",
    "    data.append(go.Histogram(\n",
    "        x=normal_batch_samples[:, i],\n",
    "        histnorm=\"probability\"\n",
    "    ))\n",
    "    \n",
    "iplot(go.Figure(data=data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shapes of the samples can be arbitrary, but the distribution they are drawn from is chosen according to the right-most index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_batch.sample((21, 17)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compute the log probability of an array of value on a batch of distribution, with the array having the same shape of the batch, we get an array of log probabilities, each from a different distribution and for the corresponding value specified.\n",
    "\n",
    "E.g. if we have three Normal distributions centered in -5, 0 and 5 respectively, and we compute the log probability of the array $(-5, 0, -5)$, we get the same value for all the entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_batch.log_prob([-5., 0., 5.]).numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, if we compute the log probability of a scalar, that's broadcast into the batch shape and the log probability is returned for the same value on each of the distributions in the batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_batch.log_prob(0.).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
